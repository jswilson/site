<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
	<meta name="generator" content="Hugo 0.57.2" />
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Jonathan Wilson &middot; 
    
  </title>

  
  <link rel="stylesheet" href="/css/poole.css">
  <link rel="stylesheet" href="/css/syntax.css">
  <link rel="stylesheet" href="/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Jonathan Wilson</p>
    <p style="margin-left: 10px">Software developer and Techstars-backed startup founder</p>
    <p style="margin-left: 10px">Contact: jswilson @ the domain of this website</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item  active " href="/">Home</a>
    <a class="sidebar-nav-item " href="/posts">Posts</a>

    
    
      
    
      
    
      
    
      
    
      
    
      
    

  </nav>

  <div class="sidebar-item">
    <p>&copy; 2019 All rights reserved.</p>
  </div>
</div>


    
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jonathan Wilson</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">





<div class="posts">
  
    <div class="post">
        <h1 class="post-title"><a href="http://example.org/posts/analyzing-pga-tour-broadcasts-part-1/">Analyzing PGA Tour Television Broadcasts, Part 1</a></h1>
        <span class="post-date">Sep 1, 2019</span>
        

<p><em>tl;dr: PGA Tour broadcasts are typically background noise for a Sunday afternoon nap, and we want to know why.  We perform some manual data labeling and machine learning to classify parts of golf broadcasts as legitimate golf-related content or not.  The early data shows that the major networks generally do broadcast an adequate amount of golf-related content, but it&rsquo;s possible that the Golf Channel does not.  We do have more to analyze in order to uncover why golf broadcasts are boring, and we plan to continue this same analysis with more data, as well as perform more advanced analyses.</em></p>

<h1 id="motivation">Motivation</h1>

<p>Professional golf on television has a reputation for being&hellip;boring.  Bleacher Report describes golf on television as <a href="https://bleacherreport.com/articles/666503-pga-tour-reading-between-the-lines-of-tv-analysts-and-commentatorss">&ldquo;white noise&rdquo;</a>.  It&rsquo;s hard to imagine someone referring to NFL games on TV in the same way&hellip;so why is golf on TV so boring?  Sure, golf is simply a less exciting game than football.  But, much like football, some golf television broadcasts are considered <a href="https://www.golf.com/news/columns/2019/06/18/why-fox-sports-has-become-the-best-tv-broadcast-in-golf/">better than others</a>.  I don&rsquo;t intend to prescribe a more exciting golf competition; I hope to determine the reasons that the current broadcasts are so unengaging as to cause viewers to sleep during them instead of watch.</p>

<h1 id="hypothesis">Hypothesis</h1>

<p>And we&rsquo;re going to test a fairly simple hypothesis to start:</p>

<p><em>“The average golf broadcast contains too much non-golf related content.”</em></p>

<p>The core of this hypothesis is that there is too much superfluous content mixed in with the golf broadcast.  Between commercials, interviews with company CEOs, video “essays”, and more, the typical golf broadcast is overrun with unrelated content to the point where it is uninteresting to watch.  Anecdotally, I would tell you these kinds of things are less pervasive in other sports broadcasts; the &ldquo;flow&rdquo; of a golf broadcast feels especially poor because of them.  So we&rsquo;re going to find out if that is the case.</p>

<h1 id="approach">Approach</h1>

<p>How can we test this hypothesis?  Doing this analysis is going to require a few steps:</p>

<h3 id="1-data-collection">1. Data Collection</h3>

<p>No matter how we approach this, we need to collect real golf television broadcast video.  We&rsquo;ll need a way to record these files to a computer for analysis after the broadcast has finished.</p>

<h3 id="2-manual-video-analysis">2. Manual Video Analysis</h3>

<p>Once we have the data collected, we&rsquo;re going to have to analyze it.  We&rsquo;ll have to define which parts of the broadcast are &ldquo;good&rdquo; and which parts are &ldquo;bad&rdquo;; we&rsquo;ll also have to find a way to label these parts for analysis.</p>

<h3 id="3-automatic-video-analysis">3. Automatic Video Analysis</h3>

<p>After we have done some initial analysis manually, we&rsquo;re going to want to perform it on an ongoing basis on larger quantities of data.  Given that this is a side project, we don&rsquo;t want to commit to manually analyzing dozens of hours of footage each week, so we&rsquo;ll need to develop an automatic method.</p>

<h1 id="data-collection">Data Collection</h1>

<p>In order to test our hypothesis, we need to analyze entire golf television broadcasts, which means we need to record golf broadcasts.  If you are unfamiliar with televised golf, professional golf tournaments (run by the PGA Tour) are played across 4 days, typically Thursday through Sunday.  Each day is a &ldquo;round&rdquo;, and a round consists of 18 holes.  Every round is broadcast on television in some  form, though the amount of television time varies depending on the importance of the tournament and the network doing the broadcast.</p>

<p>We need to record these rounds when they are on television, and we need to do it in a way such that we have a video file on a computer that we can analyze.  To accomplish this, I used a USB TV tuner; this device plugs into a computer and can receive OTA TV signals.  My particular device was a WinTV-HVR-950, but any similar device would work.  On the software side, I found <a href="https://www.nextpvr.com/">NextPVR</a> to be a great free offering for managing USB TV tuners and recording shows.  It supported my tuner, so I really just had to plug in the device, install NextPVR, and I was watching TV on my computer immediately.  This setup completely covers our requirements for data collection.</p>

<p>I scheduled recordings for all upcoming PGA Tour tournaments.  The PGA Tour season has now ended, but I recorded data for five tournaments starting July 25, 2019 and ending August 25, 2019.</p>

<h1 id="manual-video-analysis">Manual Video Analysis</h1>

<p>Once we collect the data, we have to determine how we’re going to use it to analyze the hypothesis.  How can we label these videos to test the hypothesis?  We are going to split the broadcast into 4 different classes.  Three of those classes are <em>not</em> golf-related and so should be subtracted from the total broadcast to find the golf-related content.  Our fourth class is the true golf-related content (or basically everything left over):</p>

<h3 id="commercials">Commercials</h3>

<p>Just like everything on television, golf has plenty of commercials.  But how many?  Does it have more than the average television show?  Less?  Commercials are a fairly obvious interruption to the broadcast that are definitely worth tracking.</p>

<h3 id="segments">Segments</h3>

<p>This is a broad term I’m using to encompass several things.  During the broadcast, the commentators might interview someone from the company that is sponsoring the tournament, or they might show a pre-recorded video about the city where the tournament is taking place, or they might show promotions for other television shows on the network.  These are all contained within the golf broadcast, but they are decidedly unrelated to golf and don’t tend to contribute to the enjoyment of the broadcast.  One thought is that there are too many of these, which contributes to a disjointed and boring broadcast.</p>

<h3 id="playing-through">&ldquo;Playing Through&rdquo;</h3>

<p>This is a relatively new development for golf broadcasts.  Instead of leaving the broadcast for a commercial, two video windows are shown side-by-side on the screen: one showing golf and the other showing a commercial (the audio played is for the commercial).  One could argue this is better than leaving the golf coverage altogether for a commercial, but it also could be detrimental to the quality of the broadcast if it is overused.</p>

<p><style>
  figure img {
    margin-bottom: 0;
  }
  figcaption h4 {
    font-style: italic;
    margin-top: 0;
    text-align: center;
  }
</style>
<figure>
    <img src="/playing-through.jpg"/> <figcaption>
            <h4>&#34;Playing Through&#34;: The regular broadcast is on the left, and the commercial is on the right.</h4>
        </figcaption>
</figure>
</p>

<h3 id="regular-broadcast">Regular Broadcast</h3>

<p>And finally, for purposes of this experiment, everything else leftover is considered golf-related content.  This is up for debate, of course; there are many things contained in what we&rsquo;re calling the &ldquo;regular broadcast&rdquo; that people might not enjoy.  The regular broadcast does contain all golf that is actually played, but it also contains things like player interviews and leaderboard updates.  But this is just a starting point; we&rsquo;ll test our current hypothesis and adjust what is included in future iterations.</p>

<h2 id="analysis">Analysis</h2>

<p>In order to run any sort of analysis, we need to classify the data we have into the above 4 classes: <code>commercial</code>, <code>segment</code>, <code>playing-through</code>, or <code>regular-broadcast</code>.  Each video is really a collection of frames, and we want every frame to have one of these labels associated with it.</p>

<p>There might be 15 hours of data or more per tournament, so the prospect of performing this job entirely manually is fairly unappealing.  To solve this problem, I created a special application called range-tagger.  You can learn more about range-tagger <a href="http://example.org/posts/release-of-range-tagger/">here</a>, but basically, it is designed to assist a user who needs to manually classify ranges of frames in a video.  For instance, range-tagger can help us to manually classify parts of video into sections like “Frames 0 through 1123 are a commercial, and frames 1124 through 3495 are the regular broadcast” and so on.  Fortunately, range-tagger provides approximately a 6x speedup over real-time, so accurately tagging 11 hours of video took under 2 hours.</p>

<p>Even with this speedup, manually tagging is an monotonous task, so I started by manually tagging only 3 of the videos:</p>

<table>
<thead>
<tr>
<th>Tournmament</th>
<th>Round</th>
<th>Length</th>
<th>Video Codename</th>
</tr>
</thead>

<tbody>
<tr>
<td>WGC FedEx St. Jude</td>
<td>Third Round</td>
<td>4 hours</td>
<td><code>wgc-3</code></td>
</tr>

<tr>
<td>Northern Trust Open</td>
<td>Third Round</td>
<td>3 hours</td>
<td><code>nt-3</code></td>
</tr>

<tr>
<td>Northern Trust Open</td>
<td>Fourth Round</td>
<td>4 hours</td>
<td><code>nt-4</code></td>
</tr>
</tbody>
</table>

<p>This gave us about 11 hours of accurately labeled video data at the frame-level.  Through the rest of this article, you&rsquo;ll see the videos referred to using their codename listed in the above chart.</p>

<h2 id="initial-results">Initial Results</h2>

<p>This table shows the number of frames in each video that falls into each of our 4 categories after manual labeling.  Since &ldquo;number of frames&rdquo; isn&rsquo;t a particularly intuitive number, I&rsquo;ve also included the percentage that the segment makes up of the particular video it is a part of:</p>

<table>
<thead>
<tr>
<th></th>
<th>wgc-3</th>
<th>nt-3</th>
<th>nt-4</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>commercial</code></td>
<td>69660 (16.9%)</td>
<td>61320 (19.0%)</td>
<td>65530 (15.0%)</td>
</tr>

<tr>
<td><code>segment</code></td>
<td>6870 (1.7%)</td>
<td>20820 (6.4%)</td>
<td>26580 (6.1%)</td>
</tr>

<tr>
<td><code>playing-through</code></td>
<td>7260 (1.8%)</td>
<td>8220 (2.5%)</td>
<td>8220 (1.9%)</td>
</tr>

<tr>
<td><code>regular-broadcast</code></td>
<td>327840 (79.6%)</td>
<td>233160 (72.1%)</td>
<td>336780 (77.0%)</td>
</tr>

<tr>
<td>Total Frames</td>
<td>411630</td>
<td>323520</td>
<td>437110</td>
</tr>
</tbody>
</table>

<p>The first thing that jumped out to me was that the <code>nt-3</code> video has a higher percentage of <code>commercials</code> than the other two videos.  In fact, it has almost the same amount of <em>absolute</em> commercial time as <code>nt-4</code>, which is a much longer broadcast (35.1% longer).</p>

<p>It&rsquo;s a little hard to understand these numbers just looking at a table, so I also made plots to show the timeline for each video.  Let&rsquo;s take a look at the timeline for <code>wgc-3</code> first:</p>

<figure>
    <img src="/mem-rnd3-truth.png"/> 
</figure>


<p>The x-axis is frames (which is the passage of time), and each y-axis entry is one of our 4 classes.  When a piece of a row is colored in, that means that was the class present at the given frame.  In a perfect world, the <code>regular-broadcast</code> row would be entirely solid with no gaps, but in reality the other three classes take chunks out of its timeline.</p>

<p>Let&rsquo;s put up the plots for the other two rounds and then draw some conclusions; first here is the plot for <code>nt-3</code>:</p>

<figure>
    <img src="/nt-rnd3-truth.png"/> 
</figure>


<p>And now here is the plot for <code>nt-4</code>:</p>

<figure>
    <img src="/nt-rnd4-truth.png"/> 
</figure>


<p>It&rsquo;s an interesting way to view the contents of a broadcast!  The main thing that jumped out at me visually is the large number of interruptions in the <code>nt-3</code> video.  Between <code>commercials</code>, <code>segments</code>, and <code>playing-through</code>, the broadcast is interrupted the exact same number of times (27) as the <code>wgc-3</code> video, even though the <code>wgc-3</code> video is 27.2% longer.</p>

<p>Another interesting note is the minimal use of the <code>playing-through</code> feature.  All three of videos were broadast by CBS, so they actually seem to make very modest use of it in their production.</p>

<p>These plots all use the <code>eventplot</code> feature of <code>matplotlib</code>.  You can check out the script I used for creating them <a href="https://github.com/jswilson/analyzing-pga-tour-broadcasts_scripts/blob/master/plot-broadcast-timeline.py">here</a></p>

<h2 id="manual-video-analysis-conclusions">Manual Video Analysis - Conclusions</h2>

<p>What initial conclusions can we draw from all of this?  Consider that the average 30-minute television show has about 22 minutes of actual content.  This is 73.3% real content.  The <code>nt-3</code> video contains only 72.1% regular broadcast content, which is even less than the average television show.  The other two videos do both feature more real content (79.6% and 77.0%).  The only conclusion we can draw at the moment is, <em>some</em> broadcasts <em>might</em> be of lower quality due to excessive and lengthy interruptions.  We need to collect more labeled broadcast data to see if other broadcasts perform as poorly as <code>nt-3</code>&hellip;</p>

<h1 id="automatic-video-analysis">Automatic Video Analysis</h1>

<p>&hellip;but I don&rsquo;t intend to do much more manual labeling; this task is ripe for automation.   In fact, our goal is to entirely automate the manual process we performed in the previous section.</p>

<p>And the manual labeling we performed in the previous section actually gives us a great headstart.  We now have a very nicely labeled training dataset that we can use with a supervised machine learning algorithm.  We are going to use that to train a model that will automatically recognize which of our 4 classes a given frame belongs to.</p>

<h2 id="approach-1">Approach</h2>

<p>This task is a video classification task; we have a video, and we want to classify parts of that video.  But image classification is generally much simpler, and you can turn any video classification task into an image classification by just looking at individual video frames.  Instead of starting with a more complicated video classification approach, we&rsquo;re going to start by just doing image classification and see what kind of performance we can get.</p>

<p>We already have the labels for the frames in our videos; we just need to actually extract frames to individual image files.  I wrote a small script to extract frames from a video as images which you can find <a href="https://github.com/jswilson/analyzing-pga-tour-broadcasts_scripts/blob/master/extract-frames.py">here</a>.  It does use the <code>libopenshot</code> module, which is a little tricky to compile, but you can also extract frames using command line ffmpeg without any issues.  The script extracts 1 frame for every 30 frames of video, or about 1 frame per second.</p>

<p>We can combine the extracted image files with the labels to create a dataset appropriate for use in machine learning.  I&rsquo;ve left out the code for this part, since it&rsquo;s not particularly interesting, but we just match frame numbers to labeled ranges and place them into the proper folders.  So we convert our 3 videos to images on disk, and then we use the labels to sort them into a folder structure that looks like this:</p>

<pre><code>root/
└── train/
    └── comm/
    └── regular-broadcast/
    └── pt/
    └── segment/
└── valid/
    └── comm/
    └── regular-broadcast/
    └── pt/
    └── segment/
</code></pre>

<p>For building this model, I used <code>nt-3</code> and <code>nt-4</code> as the training dataset and <code>wgc-3</code> as the validation set.  <code>nt-3</code> and <code>nt-4</code> are from the same tournament, so it&rsquo;s better to use data from a different tournament as the validation set; it takes place at a different golf course and has different players involved.</p>

<h2 id="model">Model</h2>

<p>Now that we have a labeled training dataset, we can use it to train a model.  For image classification, a great place to start is a pre-trained model.  Specifically, we&rsquo;ll use a resnet50 that has been pre-trained on ImageNet in pytorch; we&rsquo;ll start with it &ldquo;frozen,&rdquo; where we only train the final layer in the model, and then we&rsquo;ll &ldquo;unfreeze&rdquo; it and finetune all layers in model.  You can check out the script I used to train the model <a href="https://github.com/jswilson/analyzing-pga-tour-broadcasts_scripts/blob/master/train-model.py">here</a>.  This script runs a total of 9 epochs: 3 only training the final layer with learning rate of <code>0.001</code>, 3 only training the final layer with a learning rate of <code>0.0003</code>, and finally 3 finetuning the entire model with a learning rate of <code>0.0003</code>.  You can see the output here:</p>

<pre><code>Epoch 1/3
----------
train Loss: 0.3483 Acc: 0.8696
valid Loss: 0.3319 Acc: 0.8873

Epoch 2/3
----------
train Loss: 0.2683 Acc: 0.9006
valid Loss: 0.4451 Acc: 0.8485

Epoch 3/3
----------
train Loss: 0.2421 Acc: 0.9101
valid Loss: 0.4444 Acc: 0.8556

Training complete in 28m 17s
Best val Acc: 0.887326


Epoch 1/3
----------
train Loss: 0.2450 Acc: 0.9102
valid Loss: 0.3095 Acc: 0.8980

Epoch 2/3
----------
train Loss: 0.2301 Acc: 0.9159
valid Loss: 0.2953 Acc: 0.9023

Epoch 3/3
----------
train Loss: 0.2242 Acc: 0.9168
valid Loss: 0.3108 Acc: 0.8989

Training complete in 26m 43s
Best val Acc: 0.902339


Epoch 1/3
----------
train Loss: 0.1553 Acc: 0.9460
valid Loss: 0.2308 Acc: 0.9321

Epoch 2/3
----------
train Loss: 0.0708 Acc: 0.9757
valid Loss: 0.2334 Acc: 0.9279

Epoch 3/3
----------
train Loss: 0.0503 Acc: 0.9824
valid Loss: 0.1994 Acc: 0.9440

Training complete in 93m 19s
Best val Acc: 0.944027
</code></pre>

<p>Pre-trained ImageNet can definitely give some pretty remarkable results.  With very little work, we&rsquo;re getting 94.4% accuracy on our frames.  Remember, this is just on individual frames; this model receives a single image and classifies it as <code>commerical</code>, <code>segment</code>, <code>playing-through</code>, or <code>regular-broadcast</code>.  It does not use any context from previous or future frames.  So that is a great start!</p>

<h2 id="improving-the-model">Improving the Model</h2>

<p>We would like the model to be better, though.  Let&rsquo;s take a look at the plot of <code>wgc-3</code> we saw earlier, but we&rsquo;ll overlay errors as <strong>red stripes</strong>; click the image for the full resolution:</p>

<p><a href="/mem-rnd3-with-errors.png"><img src="/mem-rnd3-with-errors.png" alt="" /></a></p>

<p>A couple things are pretty clear from this plot:</p>

<h3 id="segments-aren-t-being-classified-very-well"><code>segments</code> aren&rsquo;t being classified very well</h3>

<p>Visually, the <code>segments</code> look to contain a lot of red, and this is backed up by the numbers: we only have 49.8% accuracy on <code>segment</code> frames.  There are ultimately a small number of them, and they aren&rsquo;t particularly distinct from the regular broadcast, so this is understandable.  With more training data, I suspect we could reduce the error significantly, but for now we do have trouble classifying <code>segments</code>.</p>

<h3 id="many-errors-are-isolated">Many errors are isolated</h3>

<p>Our model operates on individual frames, so this isn&rsquo;t too surprising.  Most of the time, it has no problem identifying a single <code>regular-broadcast</code> frame as being part of the regular broadcast (96.0% accuracy).  But the regular broadcast contains all kinds of content, so periodically, a frame gets incorrectly classified as something else, and it&rsquo;s fairly uncorrelated with other errors.</p>

<p>We&rsquo;re not going to try and fix the segments right now, but we can fix our second problem.  Instead of looking at just individual frames, we&rsquo;re going to group frames together.  We&rsquo;ll group <code>n</code> frames together, take the &ldquo;average&rdquo; label of those <code>n</code> frames, and say that that is the classification for that period of frames.</p>

<p>We can try this with many different values of <code>n</code>, so here is a chart trying values 1 through 90 along with the error rates we see.  Our accuracy just looking at individual frames was 94.4%, which means our base error rate was 5.6%:</p>

<figure>
    <img src="/rolling-window-size-vs-error-rate.png"/> 
</figure>


<p>This approach pretty clearly buys us a large reduction in the error rate.  At <code>n=30</code>, we see an error rate of 2.4%, a 57% improvement over just looking at individual frames.  While there are lower error rates with some larger values of <code>n</code>, they start to vary wildly with just small changes in the size of the window, so we&rsquo;ll keep <code>n=30</code>.</p>

<p>We can look at the new error plot using this approach:</p>

<p><a href="/mem-rnd3-with-errors-rolling-30.png"><img src="/mem-rnd3-with-errors-rolling-30.png" alt="" /></a></p>

<p>The widths of the bars isn&rsquo;t <em>quite</em> right in this plot (this is a little tricky to make perfectly accurate), but it still shows visually that the rolling window approach allows us to significantly reduce the classification errors.  This error rate is definitely acceptable for now, and I believe we can improve it with more training data.</p>

<h1 id="exploring-the-results">Exploring the Results</h1>

<p>So now that we have a way to automatically classify entire broadcasts into our 4 classes, we can run the model against <em>all</em> of the collected data.  There are many ways we could explore this data, but let&rsquo;s see if we can gain any insight about our initial hypothesis: <em>“the average golf broadcast contains too much non-golf related content.”</em>.  This table shows the percentage of each broadcast that is tagged with the <code>regular-broadcast</code> class, and each entry also includes the network that broadcast that particular dataset (higher <code>regular-broadcast</code> percentage is better):</p>

<p>*GC = Golf Channel</p>

<table>
<thead>
<tr>
<th>Tournament</th>
<th>Round Number</th>
<th>Channel</th>
<th>% <code>regular-broadcast</code></th>
</tr>
</thead>

<tbody>
<tr>
<td>Tour Championship</td>
<td>4</td>
<td>NBC</td>
<td>85.7%</td>
</tr>

<tr>
<td>WGC Memphis</td>
<td>3</td>
<td>CBS</td>
<td>79.8%</td>
</tr>

<tr>
<td>Tour Championship</td>
<td>2</td>
<td>GC</td>
<td>78.5%</td>
</tr>

<tr>
<td>Northern Trust</td>
<td>4</td>
<td>CBS</td>
<td>77.3%</td>
</tr>

<tr>
<td>WGC Memphis</td>
<td>4</td>
<td>CBS</td>
<td>75.8%</td>
</tr>

<tr>
<td>WGC Memphis</td>
<td>1</td>
<td>GC</td>
<td>75.8%</td>
</tr>

<tr>
<td>WGC Memphis</td>
<td>2</td>
<td>GC</td>
<td>75.2%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>3</td>
<td>NBC</td>
<td>74.6%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>2</td>
<td>GC</td>
<td>74.6%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>4</td>
<td>NBC</td>
<td>74.2%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>4</td>
<td>GC</td>
<td>74.1%</td>
</tr>

<tr>
<td>Wyndham Championship</td>
<td>1</td>
<td>GC</td>
<td>74.1%</td>
</tr>

<tr>
<td>Northern Trust</td>
<td>1</td>
<td>GC</td>
<td>74.1%</td>
</tr>

<tr>
<td><code>Average TV Show</code></td>
<td><code>-</code></td>
<td><code>-</code></td>
<td><code>73.3%</code></td>
</tr>

<tr>
<td>Northern Trust</td>
<td>2</td>
<td>GC</td>
<td>73.2%</td>
</tr>

<tr>
<td>Northern Trust</td>
<td>3</td>
<td>CBS</td>
<td>72.1%</td>
</tr>

<tr>
<td>Northern Trust</td>
<td>3</td>
<td>GC</td>
<td>71.8%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>3</td>
<td>GC</td>
<td>71.2%</td>
</tr>

<tr>
<td>Tour Championship</td>
<td>3</td>
<td>GC</td>
<td>71.1%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>1</td>
<td>GC</td>
<td>71.0%</td>
</tr>

<tr>
<td>Northern Trust</td>
<td>4</td>
<td>GC</td>
<td>70.9%</td>
</tr>

<tr>
<td>Tour Championship</td>
<td>4</td>
<td>GC</td>
<td>70.8%</td>
</tr>
</tbody>
</table>

<p><strong>A quick note on how these broadcasts work</strong>: Rounds 1 and 2 (typically on Thursday and Friday) are broadcast by the Golf Channel and not any of the major networks.  Rounds 3 and 4 (typically on Saturday and Sunday) will have early coverage from the Golf Channel, followed by coverage during the more interesting time of the tournament by one of the major networks (NBC or CBS).  This is why you might see both the Golf Channel and a major network have entries for the same round.</p>

<p>What does this table tell us?  8 out of 21 broadcasts have a lower amount of content than the average television show (73.3%).  But, 7 of these 8 are Golf Channel broadcasts; these are typically less watched than the major network broadcasts because they tend to cover less interesting parts of the tournament, plus the Golf Channel is only available with a paid subscription.</p>

<p>We can look at just the 7 broadcasts done by the major networks, which will generally have higher viewership:</p>

<table>
<thead>
<tr>
<th>Tournament</th>
<th>Round Number</th>
<th>Channel</th>
<th>% <code>regular-broadcast</code></th>
</tr>
</thead>

<tbody>
<tr>
<td>Tour Championship</td>
<td>4</td>
<td>NBC</td>
<td>85.7%</td>
</tr>

<tr>
<td>WGC Memphis</td>
<td>3</td>
<td>CBS</td>
<td>79.8%</td>
</tr>

<tr>
<td>Northern Trust</td>
<td>4</td>
<td>CBS</td>
<td>77.3%</td>
</tr>

<tr>
<td>WGC Memphis</td>
<td>4</td>
<td>CBS</td>
<td>75.8%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>3</td>
<td>NBC</td>
<td>74.6%</td>
</tr>

<tr>
<td>BMW Championship</td>
<td>4</td>
<td>NBC</td>
<td>74.2%</td>
</tr>

<tr>
<td><code>Average TV Show</code></td>
<td><code>-</code></td>
<td><code>-</code></td>
<td><code>73.3%</code></td>
</tr>

<tr>
<td>Northern Trust</td>
<td>3</td>
<td>CBS</td>
<td>72.1%</td>
</tr>
</tbody>
</table>

<p>Only 1 has less content than the average television show, and the remaining 6 contain 74.2% or more.  The fourth round of the Tour Champshionship is particularly incredible at 85.7% content; NBC delivered a large portion of this broadcast commercial free.  The one video that was worse than the average TV show was <code>nt-3</code>, which we examined in detail earlier.  With this initial limited dataset, it appears to be an outlier.</p>

<h1 id="conclusion-and-future-work">Conclusion and Future Work</h1>

<p>For the rounds of golf that most people tend to watch (those broadcast by the major networks on Saturday and Sunday), we can say that an adequate amount of real content is being shown.  They typically show more content than the average television show does.  We can&rsquo;t say that as definitively for Golf Channel broadcasts, but those aren&rsquo;t watched nearly as major as the major broadcasts.</p>

<p>What can we do to uncover more insight?  First, we&rsquo;re going to keep collecting data and analyzing it using the same process.  We only have 7 broadcasts from the major networks, and the samples we have are from especially popular tournaments.  A fuller dataset that includes a more representative sample of the broadcasts might yield more insight.  I&rsquo;ll be publishing data I collect from future tournaments on an ongoing basis.</p>

<p>But what does it mean if the percentage of <code>regular-broadcast</code> content in each broadcast is already very good?  The next step would be to take a much closer look at what is inside that <code>regular-broadcast</code> content.  We said a few different classes were bad and excluded those from the <code>regular-broadcast</code> class; but next, we&rsquo;re going to call some classes <em>good</em> and only include those as the valuable part of the broadcast.  Specifically, we&rsquo;ll look at the number of <em>golf shots</em> that are in each broadcast.  We&rsquo;ll use a more complex model to identify and count the real golf shots and see if we can learn more about what makes broadcasts boring.  Keep an eye out for the next post in this series soon.</p>

    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="http://example.org/posts/release-of-range-tagger/">Release of range-tagger v0.0.1</a></h1>
        <span class="post-date">Aug 22, 2019</span>
        

<p><em>tl;dr: range-tagger is an application to help manually label segments of long videos to create training datasets for use in machine learning.  You can download it <a href="https://github.com/jswilson/range-tagger/releases/tag/v0.0.1">here</a> (OSX only for now) and watch a demo video <a href="https://youtu.be/sZvp8YXCoto">here</a></em></p>

<h1 id="motivation">Motivation</h1>

<p>I was recently working on a machine learning project that required classifying every frame in a set of 4-hour videos.  To simplify describing the project, you can think of it like trying to classify television broadcast footage into “commercial” and “show” labels; we want to accurately label every frame in a broadcast as being an advertisement or part of the actual television show.</p>

<p>We had a collection of videos, but unfortunately none of it was labeled, so we had to create our own training data by manually labeling a portion of the videos.  What&rsquo;s the best way to go about doing this?  We didn&rsquo;t want to manually tag every single frame in the videos.  <em>&ldquo;Here is frame 4789, is it a &lsquo;commercial&rsquo; or &lsquo;show&rsquo;?  Now here&rsquo;s frame 4790&hellip;&rdquo;</em>  This is a television broadcast, so you would never have a commercial frame followed by a television frame, followed by a commercial frame.</p>

<p>Since we don&rsquo;t want to actually look at and classify every frame, it&rsquo;s easier to think of this problem as we are searching for the boundaries between parts of the video:</p>

<p><img src="/identify.png" alt="" title="Identify video boundaries" /></p>

<p>We want to end up with <em>ranges of frames</em>, each with a certain tag.  The end result should be:</p>

<table>
<thead>
<tr>
<th>Start Frame</th>
<th>End Frame</th>
<th>Tag</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>456</td>
<td>Commercial</td>
</tr>

<tr>
<td>457</td>
<td>1193</td>
<td>Show</td>
</tr>

<tr>
<td>1194</td>
<td>2035</td>
<td>Commercial</td>
</tr>

<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>

<p>This is distinct from a more common video use case, object detection.  One example of object detection might be <em>“In this video, label all of the cars.”</em>  Every frame doesn’t necessarily contain a car, or the opposite might be true depending on your source and every frame contains dozens of cars.  In our use case, we don’t need to examine every single frame like with object detection; we just need to find the “edges” of sections of the video.</p>

<p>There are standalone applications to help with object detection in videos (Microsoft has a good open source offering <a href="https://github.com/microsoft/VoTT">here</a>), but I was unable to find anything catering to my specific use case.</p>

<h1 id="initial-approach">Initial Approach</h1>

<p>Before creating range-tagger, I initially was solving the problem with a manual process.  I used freely available video players to view and navigate through the video.  I started at the beginning of the video, noted what was displayed, and then used the player’s navigation tools to move forward in the video until I found the frame at which it changed into another section.  I then recorded the frame numbers and the appropriate tag for that range in a spreadsheet.  But I ran into some problems:</p>

<h3 id="1-the-process-was-error-prone">1. The process was error prone</h3>

<p>These are long video files (4+ hours), so the frame numbers quickly became 6 digits.  It was easy to make mistakes when recording the frame numbers.  Since the purpose of this process was to generate an accurate training dataset, this was a fairly significant problem.</p>

<h3 id="2-the-freely-available-video-players-had-flaws-that-made-them-poor-fits-for-my-use-case">2. The freely available video players had flaws that made them poor fits for my use case</h3>

<p>For instance, <a href="https://www.videolan.org/vlc/index.html">VLC</a> is phenomenal for watching and consuming media, but it isn’t what I would call “frame-first.”  That is, you can&rsquo;t say &ldquo;go to this specific frame number&rdquo; or &ldquo;go forward 12 frames.&rdquo;  Traditional media players are designed for a human being consuming media, not a computer program reading specific frames.  That isn&rsquo;t the way modern video codecs are designed to work, so video players don’t provide those features.  But I <em>do</em> need to know which frame I’m on, and I do need to navigate frame-by-frame through videos.  While VLC does generally provide good coarse and fine navigation abilities, you can&rsquo;t go backwards one frame.  This lack of “frame-first” navigation made VLC a less than ideal solution.</p>

<p>I also tried an open source video editor called <a href="https://www.openshot.org/">OpenShot</a>, which is a great piece of software.  It is much more of a “frame-first” video editor; it displays frame numbers and allows you to navigate frame-by-frame, which is what I need for my use case.  But, its navigation features weren’t as fully featured as VLC’s.  There are <em>only</em> frame-forward and frame-backward keys; you can hold them down to make larger frame jumps, but it doesn’t provide coarse enough navigation abilities to quickly move through the 4+ hour videos I’m dealing with.</p>

<p>Between the process being very error prone, and no freely available solutions being “frame-first” and easy to navigate, I decided to create an application customized to my workflow.</p>

<h1 id="enter-range-tagger">Enter range-tagger</h1>

<p>range-tagger is an OSX, python, Qt-based application I created that is designed to solve these problems.  range-tagger provides:</p>

<ul>
<li>Simple tools for the user to mark segments of the video without typing frame numbers</li>
<li>Coarse and fine video navigation that work at the frame-by-frame level</li>
<li>A “frame-first” video system that knows which frame of video you are currently seeing</li>
</ul>

<p><img src="/range-tagger.jpg" alt="" title="range-tagger screenshot" /></p>

<p>I won&rsquo;t delve too much into how it works here, but you can check out a demo/how-to video at <a href="https://youtu.be/sZvp8YXCoto">this link</a>.</p>

<h2 id="technical-design-decisions">Technical Design Decisions</h2>

<p>There were a few big technical decisions that shaped how range-tagger works; I thought it would be useful to document the motivations behind them.</p>

<h3 id="web-vs-desktop">Web vs. Desktop</h3>

<p>This was really the first big decision: should range-tagger be a web app or a desktop app?  A web app would generally be preferred, since it allows anyone to get started without installation, but I ran into issues trying to identify a suitable video player library.  While there are a variety of video players available for use on the web, I was unable to find a free solution that provided frame-by-frame access and navigation.  There are paid solutions available, but one of the subgoals of this project was to offer an open source tool to the community.  Given the difficulty in solving the frame-level video problem on the web, I decided to go with a desktop application.  Sidenote: an open source, frame-by-frame web-based video player appears to be a hole in available offerings, if anyone is looking for a project.</p>

<h3 id="choice-of-video-library">Choice of Video Library</h3>

<p>Once the decision was made to make this a desktop app, I still had to choose the right library to handle video playing and navigation.  This is the API range-tagger uses internally to interface with the video library:</p>

<pre><code>    def get_total_frames(self):
        # return the total number of frames in the video

    def get_current_frame_number(self):
        # return the frame number we are currently viewing

    def get_frame(self, frame_number):
        # return the image at the given frame_number
</code></pre>

<p>This is a very simple API, which is nice because it means we can easily swap out video libraries to test them, but it was actually a little tricky to find a library that could meet it.</p>

<p>I evaluated a few different video libraries:</p>

<h3 id="libvlc">❌ libVLC</h3>

<p>The VLC developers also provide a library that performs all of the functions of VLC.  You could actually create an entire clone of the VLC application using just libVLC.  However, libVLC has the same problem as VLC: it is not “frame-first,” so you don’t know what frame you’re on, and you can’t access specific frames.</p>

<h3 id="opencv">❌ opencv</h3>

<p>Typically used for computer vision, opencv does provide mechanisms for video I/O.  I had a promising start with opencv, since the API does allow direct access to frames.  Unfortunately, the API provides <a href="https://github.com/opencv/opencv/issues/9053#issuecomment-503788660">incorrect results</a>; that is, you can use the API to ask for frame 6418, but you might not get frame 6418&hellip;you might get a different frame.  opencv is certainly a great library, but they should probably remove the API if it can’t/won’t provide the correct results.</p>

<h3 id="libopenshot">✅ libopenshot</h3>

<p>I actually hadn’t previously heard of <a href="https://github.com/OpenShot/libopenshot">libopenshot</a> until developing this application.  Much like VLC is based on libVLC, the OpenShot video editor is based entirely on libopenshot.  Their API provides frame-by-frame access to video files (and it works), so it can fulfill the required API.  I was pretty excited to stumble upon libopenshot, actually; I had been getting dismayed at the prospects of finding a workable library, but it ticks all the boxes, so this was what I chose.</p>

<h3 id="osx-only">OSX Only</h3>

<p>The only downside of libopenshot is it doesn’t appear to be too widely used, and as a result it doesn’t have prebuilt binaries available for all platforms.  Getting it to compile for my platform (OSX) was a fairly significant task, so I haven’t yet built it for Linux and Windows platforms.  This is the only dependency tying range-tagger to a single platform; I’d be happy to help anyone make a build for Linux or Windows if they have a need for it.</p>

<h1 id="results">Results</h1>

<p>With those decisions made, I was able to put together range-tagger in fairly short order.  Upon completion of the first version, I was able to label about 11 hours worth of video in under 2 hours with great accuracy and little trouble.  While range-tagger remains in alpha, it is still ready to be used for any similar workloads you might have with tagging video.</p>

<h1 id="future-work">Future work</h1>

<p>There are a couple of areas I’d like to explore in the future:</p>

<h3 id="faster-video-library">Faster video library</h3>

<p>libopenshot is a great library, and it works especially well on lower resolution video, but there can be a noticeable delay when accessing frames completely randomly in HD video, a delay that isn’t present in commercial video editors.  This is a tough problem to solve (again, modern video codecs aren’t designed for efficient random access), but I’d like to see what can be done to make the <code>get_frame</code> operation as fast as possible, even when randomly accessing frames in high resolution video.</p>

<h3 id="new-user-interface-methods-for-finding-video-edges">New user interface methods for finding video edges</h3>

<p>I started range-tagger with the simplest method: allow the user to efficiently navigate the video both coarsely and finely.  But are there other methods that could be even better?  Range-tagger can be a place to experiment with new methods of video navigation for this purpose.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Feel free to reach out on github if you use range-tagger, or if you have any interest in getting it built for Linux or Windows.  Again, you can download the first release of range tagger <a href="https://github.com/jswilson/range-tagger/releases/tag/v0.0.1">here</a>.  Enjoy!</p>

    </div>
  
</div>

<div class="pagination">
  
  <span class="pagination-item older">Older</span>
  

  
  <span class="pagination-item newer">Newer</span>
  
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  </body>
</html>

